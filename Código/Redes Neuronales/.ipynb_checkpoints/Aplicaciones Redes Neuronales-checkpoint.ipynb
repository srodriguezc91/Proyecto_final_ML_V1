{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Aplicaciones Redes Neuronales<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Clasificación-Binaria\" data-toc-modified-id=\"Clasificación-Binaria-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Clasificación Binaria</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocesamiento-de-datos\" data-toc-modified-id=\"Preprocesamiento-de-datos-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Preprocesamiento de datos</a></span></li><li><span><a href=\"#Modelo-de-clasificación-con-una-red-neuronal\" data-toc-modified-id=\"Modelo-de-clasificación-con-una-red-neuronal-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Modelo de clasificación con una red neuronal</a></span></li></ul></li><li><span><a href=\"#Clasificación-Multinomial\" data-toc-modified-id=\"Clasificación-Multinomial-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Clasificación Multinomial</a></span></li><li><span><a href=\"#Regresión-Lineal\" data-toc-modified-id=\"Regresión-Lineal-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Regresión Lineal</a></span></li><li><span><a href=\"#Conclusiones\" data-toc-modified-id=\"Conclusiones-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusiones</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso tomado de [Applied Deep Learning - Part 2: Real World Case Studies](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585#581e) de   https://towardsdatascience.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos reales de aplicación de redes neuronales\n",
    "\n",
    "Para este módulo estudiaremos tres casos diferentes de aplicación de redes neuronales. El objetivo del ejercicio es que al final tengamos clara la forma en la cual se diseña una red neuronal para resolver un problema real, cada caso consiste en una situación distinta de aplicación que utilizan el mismo modelo. Iniciamos con una clasificación binaria, luego una clasificación multiclase y finalmente una regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación Binaria\n",
    "\n",
    "Usaremos un conjunto de datos de Análisis de recursos humanos de Kaggle. La idea es predecir si un empleado dejará una compañía en función de diversas características, como la cantidad de proyectos en los que trabajó, el tiempo que pasó en la empresa, la última revisión de desempeño, el salario, entre otros. \n",
    "\n",
    "Contamos con un conjunto de datos de 15,000 registros y 9 variables, entre las que se destaca la variable \"left\" que será nuestra variable objetivo. Dicha variable admite valores 0 o 1 donde 1 significa que el empleado se ha ido.\n",
    "\n",
    "### Preprocesamiento de datos\n",
    "\n",
    "Siempre recuerde hacer un preprocesamiento de datos adecuado, la idea es responder las siguientes preguntas:\n",
    "\n",
    "* Qué tipo de características tenemos: valoradas reales, categóricas o ambas?\n",
    "* ¿Alguna de las funciones necesita normalización?\n",
    "* ¿Tenemos valores nulos?\n",
    "* ¿Cuál es la distribución de etiquetas? ¿Están las clases desequilibradas?\n",
    "* ¿Existe una correlación entre las características?\n",
    "\n",
    "Empecemos llamando las librerias importantes, creando algunas funciones útiles y finalmente cargando los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from __future__ import print_function\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import pandas_profiling as pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones útiles\n",
    "def plot_decision_boundary(func, X, y, figsize=(9, 6)):\n",
    "    amin, bmin = X.min(axis=0) - 0.1\n",
    "    amax, bmax = X.max(axis=0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "    \n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    contour = plt.contourf(aa, bb, cc, cmap=cm, alpha=0.8)\n",
    "    \n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "\n",
    "def plot_multiclass_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "    cmap = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    Z = model.predict_classes(np.c_[xx.ravel(), yy.ravel()], verbose=0)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "def plot_data(X, y, figsize=None):\n",
    "    if not figsize:\n",
    "        figsize = (8, 6)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(X[y==0, 0], X[y==0, 1], 'or', alpha=0.5, label=0)\n",
    "    plt.plot(X[y==1, 0], X[y==1, 1], 'ob', alpha=0.5, label=1)\n",
    "    plt.xlim((min(X[:, 0])-0.1, max(X[:, 0])+0.1))\n",
    "    plt.ylim((min(X[:, 1])-0.1, max(X[:, 1])+0.1))\n",
    "    plt.legend()\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
    "    loss = history.history['loss'][-1]\n",
    "    acc = history.history['accuracy'][-1]\n",
    "    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n",
    "\n",
    "def plot_loss(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, historydf.values.max()))\n",
    "    plt.title('Loss: %.3f' % history.history['loss'][-1])\n",
    "    \n",
    "def plot_confusion_matrix(model, X, y):\n",
    "    y_pred = model.predict_classes(X, verbose=0)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)), annot=True, fmt='d', cmap='GnBu', alpha=0.8, vmin=0)\n",
    "\n",
    "def plot_compare_histories(history_list, name_list, plot_accuracy=True):\n",
    "    dflist = []\n",
    "    min_epoch = len(history_list[0].epoch)\n",
    "    losses = []\n",
    "    for history in history_list:\n",
    "        h = {key: val for key, val in history.history.items() if not key.startswith('val_')}\n",
    "        dflist.append(pd.DataFrame(h, index=history.epoch))\n",
    "        min_epoch = min(min_epoch, len(history.epoch))\n",
    "        losses.append(h['loss'][-1])\n",
    "\n",
    "    historydf = pd.concat(dflist, axis=1)\n",
    "\n",
    "    metrics = dflist[0].columns\n",
    "    idx = pd.MultiIndex.from_product([name_list, metrics], names=['model', 'metric'])\n",
    "    historydf.columns = idx\n",
    "    \n",
    "    plt.figure(figsize=(6, 8))\n",
    "\n",
    "    ax = plt.subplot(211)\n",
    "    historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "    plt.title(\"Training Loss: \" + ' vs '.join([str(round(x, 3)) for x in losses]))\n",
    "    \n",
    "    if plot_accuracy:\n",
    "        ax = plt.subplot(212)\n",
    "        historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "    \n",
    "    plt.xlim(0, min_epoch-1)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def make_sine_wave():\n",
    "    c = 3\n",
    "    num = 2400\n",
    "    step = num/(c*4)\n",
    "    np.random.seed(0)\n",
    "    x0 = np.linspace(-c*np.pi, c*np.pi, num)\n",
    "    x1 = np.sin(x0)\n",
    "    noise = np.random.normal(0, 0.1, num) + 0.1\n",
    "    noise = np.sign(x1) * np.abs(noise)\n",
    "    x1  = x1 + noise\n",
    "    x0 = x0 + (np.asarray(range(num)) / step) * 0.3\n",
    "    X = np.column_stack((x0, x1))\n",
    "    y = np.asarray([int((i/step)%2==1) for i in range(len(x0))])\n",
    "    return X, y\n",
    "\n",
    "def make_multiclass(N=500, D=2, K=3):\n",
    "    \"\"\"\n",
    "    N: number of points per class\n",
    "    D: dimensionality\n",
    "    K: number of classes\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    X = np.zeros((N*K, D))\n",
    "    y = np.zeros(N*K)\n",
    "    for j in range(K):\n",
    "        ix = range(N*j, N*(j+1))\n",
    "        # radius\n",
    "        r = np.linspace(0.0,1,N)\n",
    "        # theta\n",
    "        t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "    plt.xlim([-1,1])\n",
    "    plt.ylim([-1,1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos\n",
    "DF=pd.read_csv('HR.csv')\n",
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respondamos la primera pregunta, tipo de datos y resumen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una inspección rápida para saber si debemos estandarizar los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos sus correlaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(DF.corr(), annot=True, square=True, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos la distribución de cada una de las variables para ver cuáles necesitan estandarización. Selccionamos para ello \n",
    "\n",
    "* average_monthly_hours,\n",
    "* number_project y \n",
    "* time_spend_company. \n",
    "\n",
    "¿Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp.ProfileReport(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DF.copy()\n",
    "\n",
    "ss = StandardScaler()\n",
    "scale_features = ['average_monthly_hours', 'number_project', 'time_spend_company']\n",
    "df[scale_features] = ss.fit_transform(df[scale_features])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, estamos muy contentos con nuestra base estandarizada, sin embargo nos enfrentamos a otro problema, las variables categóricas. Recuerden que cuando se definió el perceptrón lo hicimos como un elemento matemático y como tal debe tener entradas numéricas, no obstante, como lo vimos en clases pasadas entendimos que una forma de volver numérica una variable categórica es usando variables dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['sales', 'salary']\n",
    "df_cat = pd.get_dummies(df[categorical_features])\n",
    "df = df.drop(categorical_features, axis=1)\n",
    "df = pd.concat([df, df_cat], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos están listos pero falta la partición de la base en entrenamiento y testeo, como tenemos una base generosa utilizaremos el 30% de los registros para hacer testeo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('left', axis=1).values\n",
    "y = df['left'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de clasificación con una red neuronal\n",
    "\n",
    "Ahora vamos a crear un modelo para predecir la variable left, aprovecharemois el desorden para jugar un poco con la tasa de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_clas = Sequential()\n",
    "bin_clas.add(Dense(64, input_shape=(X_train.shape[1],), activation='tanh'))\n",
    "bin_clas.add(Dense(16, activation='tanh'))\n",
    "bin_clas.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "bin_clas.compile(Adam(lr=0.1), 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bin_hist = bin_clas.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(bin_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con una tasa de aprendizaje no tan complicada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_clas = Sequential()\n",
    "bin_clas.add(Dense(64, input_shape=(X_train.shape[1],), activation='tanh'))\n",
    "bin_clas.add(Dense(32, input_shape=(X_train.shape[1],), activation='tanh'))\n",
    "bin_clas.add(Dense(16, activation='tanh'))\n",
    "bin_clas.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "bin_clas.compile(Adam(lr=0.005), 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bin_hist = bin_clas.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(bin_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buena aproximación, grafiquemos la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bin_clas.predict_classes(X_test, verbose=0)\n",
    "print(classification_report(y_test, y_pred))\n",
    "plot_confusion_matrix(bin_clas, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como nos fue en la de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bin_clas.predict_classes(X_train, verbose=0)\n",
    "print(classification_report(y_train, y_pred))\n",
    "plot_confusion_matrix(bin_clas, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación Multinomial\n",
    "\n",
    "Ahora trabajaremos con otra base [Car Evaluation DataBase](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) se derivó de un modelo de decisión jerárquico simple desarrollado originalmente para la demostración de DEX, M. Bohanec, V. Rajkovic: Sistema experto para la toma de decisiones. Sistemica 1 (1), págs. 145-157, 1990.). El modelo evalúa los coches de acuerdo con la siguiente estructura conceptual:\n",
    "\n",
    "Aceptabilidad del coche CAR\n",
    "* PRECIO precio total\n",
    "        buying : precio de compra\n",
    "        maint price: precio de mantenimiento\n",
    "* Características técnicas TECH\n",
    "* CONFORT:  comodidad\n",
    "        doors:  número de puertas\n",
    "        persons capacity:  personas para llevar\n",
    "        lug_boot: el tamaño del maletero\n",
    "        safety:  seguridad estimada del coche\n",
    "\n",
    "La base de datos relaciona directamente el automóvil con los seis atributos de entrada: compra, mantenimiento, puertas, personas, maletero, seguridad.\n",
    "\n",
    "Debido a la estructura de concepto subyacente conocida, esta base de datos puede ser particularmente útil para probar métodos de inducción constructiva y descubrimiento de estructuras.\n",
    "\n",
    "Información de atributos:\n",
    "\n",
    "Valores de clase:\n",
    "\n",
    "unacc, acc, good, vgood\n",
    "\n",
    "Atributos:\n",
    "\n",
    "* buying: vhigh, high, med, low.\n",
    "* maint: vhigh, high, med, low.\n",
    "* doors: 2, 3, 4, 5 or more.\n",
    "* persons: 2, 4, more.\n",
    "* lug_boot: small, med, big.\n",
    "* safety: low, med, high.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2=pd.read_csv('car.data',header=None)\n",
    "DF2.columns=['buying','maint','doors','persons','lug_boot','safety','clas']\n",
    "DF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=DF2.copy()\n",
    "categorical_features=df.columns[:-1]\n",
    "df_cat = pd.get_dummies(df[categorical_features])\n",
    "df.drop(categorical_features, axis=1)\n",
    "df = pd.concat([ df_cat,df['clas']], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como las variables de entrada son categóricas no hacemos ninguna estandarización, iniciamos planteando el modelo, en primer lugar adecuamos la bases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(df.values[:, :-1]).astype('float32')\n",
    "y = np.asarray(pd.get_dummies(df['clas']).values).astype('float32')\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definimos entrenamiento y testeo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente definimos un modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_clas = Sequential()\n",
    "mul_clas.add(Dense(32, input_shape=(X.shape[1],), activation='relu'))\n",
    "mul_clas.add(Dense(16, activation='relu'))\n",
    "mul_clas.add(Dense(8, activation='relu'))\n",
    "mul_clas.add(Dense(4, activation='softmax'))\n",
    "mul_clas.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = mul_clas.fit(X_train, y_train, epochs=16, verbose=0)\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = mul_clas.predict_classes(X_test, verbose=0)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "plot_confusion_matrix(mul_clas, X_test, y_test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "Usando la sintáxis de las redes neuronales modelaremos una regresión lineal, utilizaremos los datos de Kaggle para el caso [House Sales in King County, USA](https://www.kaggle.com/harlfoxem/housesalesprediction).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF3=pd.read_csv('houses_price.csv',index_col='id')\n",
    "DF3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos una rápida exploración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp.ProfileReport(DF3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la correlación entre variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 24))\n",
    "sns.heatmap(DF3.corr(), annot=True, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las características relacionadas con pies cuadrados definitivamente deben estandarizarse, ya que los valores varían en miles y las características como el código postal deben clasificarse.\n",
    "\n",
    "También tenemos un nuevo tipo de preprocesamiento que realizar, *bucketization*. Por ejemplo, la característica que contiene el año en que se construyó la casa (yr_built), varía de 1900 a 2015. Ciertamente podemos clasificarla con cada año perteneciente a una categoría distinta, pero entonces sería bastante escasa. Obtendríamos más señal si agrupamos esta función sin perder mucha información. Por ejemplo, si usamos cubos de 10 años, los años entre [1950, 1959] se colapsarían juntos. Probablemente sería suficiente saber que esta casa fue construida en la década de 1950 en lugar de en 1958 exactamente.\n",
    "\n",
    "Otras características que se beneficiarían de la *bucketization* son la latitud y la longitud de la casa. La coordenada exacta no importa tanto, podemos redondear la coordenada al kilómetro más cercano. De esta manera, los valores de las características serán más densos e informativos. No existe una regla fija y estricta sobre los rangos que se deben usar en la agrupación en segmentos, se deciden principalmente por prueba y error.\n",
    "\n",
    "Una última transformación que debemos hacer es el precio de la casa, actualmente su valor oscila entre $\\$ 75 K$ y $\\$ 7.7 M$. Un modelo que intenta predecir a una escala y variación tan grande sería muy inestable. Así que también normalizamos eso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DF3.copy()\n",
    "\n",
    "# Estandarizaciones\n",
    "ss = StandardScaler()\n",
    "scale_features = ['bathrooms', 'bedrooms', 'grade', 'sqft_above', \n",
    "                  'sqft_basement', 'sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_lot15']\n",
    "df[scale_features] = ss.fit_transform(df[scale_features])\n",
    "\n",
    "# bucketization\n",
    "bucketized_features = ['yr_built', 'yr_renovated', 'lat', 'long']\n",
    "\n",
    "bins = range(1890, 2021, 10)\n",
    "df['yr_built'] = pd.cut(df.yr_built, bins, labels=bins[:-1])\n",
    "\n",
    "bins = list(range(1930, 2021, 10))\n",
    "bins = [-10] + bins\n",
    "df['yr_renovated'] = pd.cut(df.yr_renovated, bins, labels=bins[:-1])\n",
    "\n",
    "bins = np.arange(47.00, 47.90, 0.05)\n",
    "df['lat'] = pd.cut(df.lat, bins, labels=bins[:-1])\n",
    "\n",
    "bins = np.arange(-122.60, -121.10, 0.05)\n",
    "df['long'] = pd.cut(df.long, bins, labels=bins[:-1])\n",
    "\n",
    "# Variables categóricas\n",
    "df['date'] = [datetime.strptime(x, '%Y%m%dT000000').strftime('%Y-%m') for x in DF3['date'].values]\n",
    "df['zipcode'] = df['zipcode'].astype('str')\n",
    "categorical_features = ['zipcode', 'date']\n",
    "categorical_features.extend(bucketized_features)\n",
    "df_cat = pd.get_dummies(df[categorical_features])\n",
    "df = df.drop(categorical_features, axis=1)\n",
    "df = pd.concat([df, df_cat], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos preparamos para hacer el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['price'], axis=1).values\n",
    "y = df['price'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Arreglo de outliers\n",
    "factor = 5\n",
    "y_train[np.abs(y_train - y_train.mean()) > (factor * y_train.std())] = y_train.mean() + factor*y_train.std()\n",
    "\n",
    "# Estandarizar precio\n",
    "ss_price = StandardScaler()\n",
    "y_train = ss_price.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = ss_price.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo lineal no tendrá función de activación y la función de pérdida será el Error cuadrático medio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linr_model = Sequential()\n",
    "linr_model.add(Dense(1, input_shape=(X.shape[1],)))\n",
    "\n",
    "linr_model.compile('adam', 'mean_squared_error')\n",
    "\n",
    "linr_history = linr_model.fit(X_train, y_train, epochs=30, verbose=0, validation_split=0.2)\n",
    "plot_loss(linr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linr_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coeficientes\n",
    "linr_wdf = pd.DataFrame(linr_model.get_weights()[0].T, \n",
    "                      columns=df.drop(['price'], axis=1).columns).T.sort_values(0, ascending=False)\n",
    "linr_wdf.columns = ['feature_weight']\n",
    "linr_wdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente construimos una Red Neuronal para la regresión. En los ejemplos anteriores, pasar de un modelo lineal a un modelo profundo solo implicaba agregar nuevas capas con funciones de activación no lineales. Será lo mismo esta vez también.\n",
    "\n",
    "Agregamos nuevas capas con activación relu. La trama de pérdidas parece interesante ahora. La pérdida de error de entrenamiento todavía parece estar disminuyendo, pero el error de validación comienza a aumentar después de la quinta época. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = Sequential()\n",
    "deep_model.add(Dense(32, input_shape=(X.shape[1],), activation='relu'))\n",
    "deep_model.add(Dense(16, activation='relu'))\n",
    "deep_model.add(Dense(8, activation='relu'))\n",
    "deep_model.add(Dense(1))\n",
    "\n",
    "deep_model.compile('adam', 'mean_squared_error')\n",
    "\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "deep_history = deep_model.fit(X_train, y_train, epochs=30, verbose=0, validation_split=0.2)\n",
    "#                               callbacks=[early_stop])\n",
    "plot_loss(deep_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente estamos sobreajustando. La ANN está memorizando los datos de entrenamiento y esto está reduciendo su capacidad de generalizar en el conjunto de validación.\n",
    "\n",
    "La regularización es un método creado para evitar el sobreajuste, sin embargo, podemos pensar en uno más simple, parar el procesamiento cuando se empiece a sentir el cambio entre la medida de las funciones de pérdida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con early stopping\n",
    "deep_model = Sequential()\n",
    "deep_model.add(Dense(32, input_shape=(X.shape[1],), activation='relu'))\n",
    "deep_model.add(Dense(16, activation='relu'))\n",
    "deep_model.add(Dense(8, activation='relu'))\n",
    "deep_model.add(Dense(1))\n",
    "\n",
    "deep_model.compile('adam', 'mean_squared_error')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "deep_history = deep_model.fit(X_train, y_train, epochs=30, verbose=0, validation_split=0.2,\n",
    "                              callbacks=[early_stop])\n",
    "plot_loss(deep_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare_histories([linr_history, deep_history], ['Linear Reg', 'Deep ANN'], plot_accuracy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_dollars(num):\n",
    "    return '$'+str(\"{:,}\".format(int(num)))\n",
    "\n",
    "print('Average house price:', output_dollars(DF3['price'].mean()))\n",
    "\n",
    "real_prices = ss_price.inverse_transform(y_test)\n",
    "\n",
    "# print('Training set house price:', np.mean(ss_price.inverse_transform(y_train)))\n",
    "\n",
    "train_prices = ss_price.inverse_transform(y_train)\n",
    "dumb_prices = np.zeros(real_prices.shape)\n",
    "dumb_prices.fill(train_prices.mean())\n",
    "dumb_error = mean_absolute_error(real_prices, dumb_prices)\n",
    "print('Dumb model error:', output_dollars(dumb_error))\n",
    "\n",
    "linr_predictions = linr_model.predict(X_test).ravel()\n",
    "linr_prices = ss_price.inverse_transform(linr_predictions)\n",
    "linr_error = mean_absolute_error(real_prices, linr_prices)\n",
    "print('Linear model error:', output_dollars(linr_error))\n",
    "\n",
    "deep_predictions = deep_model.predict(X_test).ravel()\n",
    "deep_prices = ss_price.inverse_transform(deep_predictions)\n",
    "deep_error = mean_absolute_error(real_prices, deep_prices)\n",
    "print('Deep model error:', output_dollars(deep_error))\n",
    "\n",
    "tdf = pd.DataFrame([['Naive Model', output_dollars(dumb_error)], \n",
    "                    ['Linear Regression', output_dollars(linr_error)], \n",
    "                    ['Deep ANN', output_dollars(deep_error)]], \n",
    "                   columns=['Model', 'Price Error'])\n",
    "tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Usamos las redes neuronales para tres tipos de problemas típicos del Machine Learning, evidentemente el ajuste del modelo a los datos del entrenamiento produce potro gran lio, el sobreajuste, veremos como enfrentarlo en otro ejercicio. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Aplicaciones Redes Neuronales",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
